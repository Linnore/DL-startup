{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This notebook prepare the cifar10 dataset on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the cifar10 dataset.\n",
    "Use vpn or eduroam if connection timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='../datasets/', train=True, download=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='../datasets/', download=True)\n",
    "cifar_root = '../datasets/cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../datasets/\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "print(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "loc_1 = '../datasets/cifar-10-batches-py/train'\n",
    "loc_2 = '../datasets/cifar-10-batches-py/test'\n",
    "if not os.path.exists(loc_1):\n",
    "    os.mkdir(loc_1)\n",
    "if not os.path.exists(loc_2):\n",
    "    os.mkdir(loc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(cifar_root):\n",
    "    meta = unpickle(os.path.join(cifar_root, 'batches.meta'))\n",
    "    label_names = meta[b'label_names']\n",
    "    for i in label_names:\n",
    "        dir1 = loc_1 + '/' + i.decode()\n",
    "        dir2 = loc_2 + '/' + i.decode()\n",
    "        if not os.path.exists(dir1):\n",
    "            os.mkdir(dir1)\n",
    "        if not os.path.exists(dir2):\n",
    "            os.mkdir(dir2)\n",
    "\n",
    "    for i in range(1,6):\n",
    "        data_name = 'data_batch_' + str(i)\n",
    "        data = unpickle(os.path.join(cifar_root, data_name))\n",
    "        for j in range (10000):\n",
    "            img = np.reshape(data[b'data'][j], (3, 32, 32))\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            label = label_names[data[b'labels'][j]].decode()\n",
    "            img_name = label + '_' + str(i*10000 + j) + '.jpg'\n",
    "            img_save_path = loc_1 + '/' + label + '/' + img_name\n",
    "            cv2.imwrite(img_save_path, img)\n",
    "        print(data_name + ' finished')\n",
    "\n",
    "    test_data = unpickle(os.path.join(cifar_root, 'test_batch'))\n",
    "    for i in range (10000):\n",
    "            img = np.reshape(test_data[b'data'][i], (3, 32, 32))\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            label = label_names[test_data[b'labels'][i]].decode()\n",
    "            img_name = label + '_' + str(i) + '.jpg'\n",
    "            img_save_path = loc_2 + '/' + label + '/' + img_name\n",
    "            cv2.imwrite(img_save_path, img)\n",
    "    print('test_batch finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_batch_1 finished\n",
      "data_batch_2 finished\n",
      "data_batch_3 finished\n",
      "data_batch_4 finished\n",
      "data_batch_5 finished\n",
      "test_batch finished\n"
     ]
    }
   ],
   "source": [
    "unzip(cifar_root=cifar_root)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f8619f41d5efcc50a81907634beda274afdfd382954cddbd8ddea9703d58041"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('i2dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
